{"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/audioui\/realtimeaudio"]}],"sections":[],"primaryContentSections":[{"kind":"content","content":[{"anchor":"Overview","type":"heading","level":2,"text":"Overview"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Real-time audio applications require interfaces that update smoothly without interfering with audio processing. AudioUI provides specialized patterns and components designed for the unique demands of real-time audio systems."}]},{"anchor":"Real-Time-Requirements","type":"heading","level":2,"text":"Real-Time Requirements"},{"anchor":"Timing-Constraints","type":"heading","level":3,"text":"Timing Constraints"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Audio processing operates under strict timing requirements:"}]},{"items":[{"content":[{"inlineContent":[{"inlineContent":[{"text":"Audio Buffer Size","type":"text"}],"type":"strong"},{"type":"text","text":": Typically 64-512 samples"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"inlineContent":[{"type":"text","text":"Sample Rate"}],"type":"strong"},{"text":": 44.1kHz to 192kHz","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"strong","inlineContent":[{"type":"text","text":"Latency Target"}]},{"type":"text","text":": < 10ms for live performance"}]}]},{"content":[{"type":"paragraph","inlineContent":[{"inlineContent":[{"text":"UI Update Rate","type":"text"}],"type":"strong"},{"type":"text","text":": 60-120 FPS without audio dropouts"}]}]}],"type":"unorderedList"},{"anchor":"Thread-Separation","type":"heading","level":3,"text":"Thread Separation"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Critical separation between audio processing and UI updates:"}]},{"code":["import AudioUI","import AVFoundation","","class RealTimeAudioEngine: ObservableObject {","    \/\/ Audio processing thread (high priority)","    private var audioQueue = DispatchQueue(","        label: \"audio.processing\",","        qos: .userInteractive,","        attributes: .concurrent","    )","    ","    \/\/ UI update thread (main thread)","    @Published var currentLevel: Float = 0","    @Published var frequency: Float = 440","    @Published var isProcessing: Bool = false","    ","    private var audioEngine: AVAudioEngine","    private var playerNode: AVAudioPlayerNode","    ","    init() {","        audioEngine = AVAudioEngine()","        playerNode = AVAudioPlayerNode()","        setupAudioGraph()","        setupRealTimeUpdates()","    }","    ","    private func setupRealTimeUpdates() {","        \/\/ Audio thread provides data","        audioQueue.async {","            while self.isProcessing {","                let level = self.calculateCurrentLevel()","                let freq = self.calculateDominantFrequency()","                ","                \/\/ UI updates on main thread","                DispatchQueue.main.async {","                    self.currentLevel = level","                    self.frequency = freq","                }","                ","                \/\/ Control update rate","                Thread.sleep(forTimeInterval: 1\/60)","            }","        }","    }","}"],"type":"codeListing","syntax":"swift"},{"anchor":"Lock-Free-Communication","type":"heading","level":2,"text":"Lock-Free Communication"},{"anchor":"Atomic-Updates","type":"heading","level":3,"text":"Atomic Updates"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Use atomic operations for thread-safe communication:"}]},{"type":"codeListing","code":["import Foundation","","class AtomicFloat {","    private var _value: UnsafeMutablePointer<Float>","    ","    init(_ value: Float = 0) {","        _value = UnsafeMutablePointer<Float>.allocate(capacity: 1)","        _value.initialize(to: value)","    }","    ","    deinit {","        _value.deallocate()","    }","    ","    var value: Float {","        get {","            return _value.pointee","        }","        set {","            _value.pointee = newValue","        }","    }","    ","    func atomicLoad() -> Float {","        return OSAtomicAdd32(0, _value)","    }","    ","    func atomicStore(_ value: Float) {","        OSAtomicCompareAndSwap32(_value.pointee, value, _value)","    }","}","","class RealTimeMeter: ObservableObject {","    private let atomicLevel = AtomicFloat()","    @Published var displayLevel: Float = 0","    ","    \/\/ Called from audio thread","    func updateFromAudioThread(_ level: Float) {","        atomicLevel.atomicStore(level)","    }","    ","    \/\/ Called from UI thread","    func updateDisplay() {","        displayLevel = atomicLevel.atomicLoad()","    }","}"],"syntax":"swift"},{"type":"heading","level":3,"text":"Ring Buffers","anchor":"Ring-Buffers"},{"type":"paragraph","inlineContent":[{"text":"Efficient data sharing between threads:","type":"text"}]},{"type":"codeListing","code":["class RingBuffer<T> {","    private var buffer: [T]","    private var capacity: Int","    private var writeIndex: Int = 0","    private var readIndex: Int = 0","    private let lock = NSLock()","    ","    init(capacity: Int, defaultValue: T) {","        self.capacity = capacity","        self.buffer = Array(repeating: defaultValue, count: capacity)","    }","    ","    func write(_ value: T) -> Bool {","        lock.lock()","        defer { lock.unlock() }","        ","        let nextWrite = (writeIndex + 1) % capacity","        if nextWrite == readIndex {","            return false \/\/ Buffer full","        }","        ","        buffer[writeIndex] = value","        writeIndex = nextWrite","        return true","    }","    ","    func read() -> T? {","        lock.lock()","        defer { lock.unlock() }","        ","        if readIndex == writeIndex {","            return nil \/\/ Buffer empty","        }","        ","        let value = buffer[readIndex]","        readIndex = (readIndex + 1) % capacity","        return value","    }","}","","class AudioDataBridge: ObservableObject {","    private let levelBuffer = RingBuffer<Float>(capacity: 1024, defaultValue: 0)","    @Published var currentLevels: [Float] = []","    ","    \/\/ Audio thread writes","    func pushLevel(_ level: Float) {","        _ = levelBuffer.write(level)","    }","    ","    \/\/ UI thread reads","    func updateUI() {","        var levels: [Float] = []","        while let level = levelBuffer.read() {","            levels.append(level)","        }","        ","        if !levels.isEmpty {","            currentLevels = levels","        }","    }","}"],"syntax":"swift"},{"type":"heading","level":2,"text":"Performance-Optimized Components","anchor":"Performance-Optimized-Components"},{"type":"heading","level":3,"text":"Efficient Level Meters","anchor":"Efficient-Level-Meters"},{"type":"paragraph","inlineContent":[{"text":"Level meters optimized for real-time updates:","type":"text"}]},{"type":"codeListing","code":["struct RealTimeLevelMeter: View {","    let level: Float","    let peak: Float","    ","    \/\/ Cache drawing calculations","    @State private var segmentCount: Int = 20","    @State private var segmentHeight: CGFloat = 0","    @State private var segmentSpacing: CGFloat = 1","    ","    var body: some View {","        GeometryReader { geometry in","            Canvas { context, size in","                updateSegmentCalculations(size: size)","                drawOptimizedMeter(context: context, size: size)","            }","            .drawingGroup() \/\/ Composite to single layer","        }","    }","    ","    private func updateSegmentCalculations(size: CGSize) {","        segmentHeight = (size.height - CGFloat(segmentCount - 1) * segmentSpacing) \/ CGFloat(segmentCount)","    }","    ","    private func drawOptimizedMeter(context: GraphicsContext, size: CGSize) {","        let activeSegments = Int(level * Float(segmentCount))","        let peakSegment = Int(peak * Float(segmentCount))","        ","        for i in 0..<segmentCount {","            let y = size.height - CGFloat(i + 1) * (segmentHeight + segmentSpacing)","            let rect = CGRect(x: 0, y: y, width: size.width, height: segmentHeight)","            ","            let color: Color","            if i < activeSegments {","                color = segmentColor(for: i, total: segmentCount)","            } else if i == peakSegment {","                color = .yellow","            } else {","                color = .gray.opacity(0.3)","            }","            ","            context.fill(Path(roundedRect: rect, cornerRadius: 1), with: .color(color))","        }","    }","    ","    private func segmentColor(for index: Int, total: Int) -> Color {","        let ratio = Float(index) \/ Float(total)","        if ratio < 0.7 {","            return .green","        } else if ratio < 0.9 {","            return .yellow","        } else {","            return .red","        }","    }","}"],"syntax":"swift"},{"type":"heading","level":3,"text":"Batch Update Manager","anchor":"Batch-Update-Manager"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Coordinate multiple component updates efficiently:"}]},{"type":"codeListing","code":["class RealTimeUpdateManager: ObservableObject {","    @Published var meterLevels: [Float] = Array(repeating: 0, count: 32)","    @Published var knobValues: [Float] = Array(repeating: 0.5, count: 16)","    @Published var buttonStates: [Bool] = Array(repeating: false, count: 8)","    ","    private var pendingUpdates: Set<UpdateType> = []","    private var updateTimer: Timer?","    ","    enum UpdateType {","        case meters, knobs, buttons","    }","    ","    init() {","        startBatchUpdates()","    }","    ","    private func startBatchUpdates() {","        updateTimer = Timer.scheduledTimer(withTimeInterval: 1\/60) { _ in","            self.processPendingUpdates()","        }","    }","    ","    \/\/ Called from audio thread","    func scheduleUpdate(_ type: UpdateType) {","        DispatchQueue.main.async {","            self.pendingUpdates.insert(type)","        }","    }","    ","    private func processPendingUpdates() {","        guard !pendingUpdates.isEmpty else { return }","        ","        \/\/ Batch all updates into single UI refresh","        for updateType in pendingUpdates {","            switch updateType {","            case .meters:","                updateMeterLevels()","            case .knobs:","                updateKnobValues()","            case .buttons:","                updateButtonStates()","            }","        }","        ","        pendingUpdates.removeAll()","    }","}"],"syntax":"swift"},{"type":"heading","level":2,"text":"Low-Latency Patterns","anchor":"Low-Latency-Patterns"},{"type":"heading","level":3,"text":"Direct Hardware Access","anchor":"Direct-Hardware-Access"},{"type":"paragraph","inlineContent":[{"text":"For critical applications requiring minimal latency:","type":"text"}]},{"type":"codeListing","code":["import CoreAudio","import AudioToolbox","","class UltraLowLatencyEngine {","    private var audioUnit: AudioComponentInstance?","    private var sampleRate: Double = 44100","    private var bufferSize: UInt32 = 64","    ","    private let uiUpdateCallback: (Float) -> Void","    ","    init(uiUpdateCallback: @escaping (Float) -> Void) {","        self.uiUpdateCallback = uiUpdateCallback","        setupAudioUnit()","    }","    ","    private func setupAudioUnit() {","        var description = AudioComponentDescription(","            componentType: kAudioUnitType_Output,","            componentSubType: kAudioUnitSubType_HALOutput,","            componentManufacturer: kAudioUnitManufacturer_Apple,","            componentFlags: 0,","            componentFlagsMask: 0","        )","        ","        guard let component = AudioComponentFindNext(nil, &description) else {","            return","        }","        ","        AudioComponentInstanceNew(component, &audioUnit)","        ","        \/\/ Set render callback","        var callbackStruct = AURenderCallbackStruct(","            inputProc: audioRenderCallback,","            inputProcRefCon: UnsafeMutableRawPointer(Unmanaged.passUnretained(self).toOpaque())","        )","        ","        AudioUnitSetProperty(","            audioUnit!,","            kAudioUnitProperty_SetRenderCallback,","            kAudioUnitScope_Input,","            0,","            &callbackStruct,","            UInt32(MemoryLayout<AURenderCallbackStruct>.size)","        )","    }","    ","    private let audioRenderCallback: AURenderCallback = { (","        inRefCon,","        ioActionFlags,","        inTimeStamp,","        inBusNumber,","        inNumberFrames,","        ioData","    ) -> OSStatus in","        let engine = Unmanaged<UltraLowLatencyEngine>.fromOpaque(inRefCon).takeUnretainedValue()","        return engine.renderAudio(ioData: ioData!, frameCount: inNumberFrames)","    }","    ","    private func renderAudio(ioData: UnsafeMutablePointer<AudioBufferList>, frameCount: UInt32) -> OSStatus {","        \/\/ Process audio here","        let level = calculateLevel(from: ioData, frameCount: frameCount)","        ","        \/\/ Ultra-low latency UI update","        DispatchQueue.main.async {","            self.uiUpdateCallback(level)","        }","        ","        return noErr","    }","}"],"syntax":"swift"},{"type":"heading","level":3,"text":"Efficient State Management","anchor":"Efficient-State-Management"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Minimize state changes and updates:"}]},{"type":"codeListing","code":["class EfficiientAudioState: ObservableObject {","    \/\/ Group related values to minimize update notifications","    struct MeterState {","        var level: Float","        var peak: Float","        var clip: Bool","    }","    ","    struct ControlState {","        var frequency: Float","        var resonance: Float","        var cutoff: Float","    }","    ","    @Published var meterState = MeterState(level: 0, peak: 0, clip: false)","    @Published var controlState = ControlState(frequency: 440, resonance: 0.5, cutoff: 0.7)","    ","    \/\/ Batch updates to reduce redraws","    func updateMeterValues(level: Float, peak: Float, clip: Bool) {","        let newState = MeterState(level: level, peak: peak, clip: clip)","        ","        \/\/ Only update if values changed significantly","        if shouldUpdate(newState, from: meterState) {","            meterState = newState","        }","    }","    ","    private func shouldUpdate(_ new: MeterState, from old: MeterState) -> Bool {","        return abs(new.level - old.level) > 0.01 ||","               abs(new.peak - old.peak) > 0.01 ||","               new.clip != old.clip","    }","}"],"syntax":"swift"},{"type":"heading","level":2,"text":"Audio-Driven Animations","anchor":"Audio-Driven-Animations"},{"type":"heading","level":3,"text":"Responsive UI Elements","anchor":"Responsive-UI-Elements"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Create interfaces that feel connected to the audio:"}]},{"type":"codeListing","code":["struct AudioResponsiveKnob: View {","    @Binding var value: Double","    let audioLevel: Float","    ","    @State private var pulseScale: CGFloat = 1.0","    @State private var glowOpacity: Double = 0.0","    ","    var body: some View {","        InsetNeumorphicKnob(value: $value)","            .scaleEffect(pulseScale)","            .shadow(","                color: .blue.opacity(glowOpacity),","                radius: 20","            )","            .onChange(of: audioLevel) { level in","                \/\/ Respond to audio with subtle animation","                withAnimation(.easeOut(duration: 0.1)) {","                    pulseScale = 1.0 + CGFloat(level) * 0.1","                    glowOpacity = Double(level) * 0.5","                }","            }","    }","}"],"syntax":"swift"},{"type":"heading","level":3,"text":"Beat-Synchronized Effects","anchor":"Beat-Synchronized-Effects"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Sync UI elements to musical timing:"}]},{"type":"codeListing","code":["class BeatDetector: ObservableObject {","    @Published var beatDetected: Bool = false","    @Published var tempo: Float = 120.0","    ","    private var audioEngine: AudioEngine","    private var lastBeatTime: CFAbsoluteTime = 0","    ","    init(audioEngine: AudioEngine) {","        self.audioEngine = audioEngine","        startBeatDetection()","    }","    ","    private func startBeatDetection() {","        Timer.scheduledTimer(withTimeInterval: 1\/60) { _ in","            self.analyzeBeat()","        }","    }","    ","    private func analyzeBeat() {","        let currentTime = CFAbsoluteTimeGetCurrent()","        let level = audioEngine.kickLevel","        ","        \/\/ Simple beat detection","        if level > 0.8 && (currentTime - lastBeatTime) > 0.3 {","            lastBeatTime = currentTime","            ","            DispatchQueue.main.async {","                self.beatDetected = true","                ","                \/\/ Reset beat state","                DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {","                    self.beatDetected = false","                }","            }","        }","    }","}","","struct BeatSyncButton: View {","    @ObservedObject var beatDetector: BeatDetector","    let action: () -> Void","    ","    @State private var beatScale: CGFloat = 1.0","    ","    var body: some View {","        CircularButton(action: action)","            .scaleEffect(beatScale)","            .onChange(of: beatDetector.beatDetected) { detected in","                if detected {","                    withAnimation(.easeOut(duration: 0.2)) {","                        beatScale = 1.2","                    }","                    withAnimation(.easeOut(duration: 0.3).delay(0.1)) {","                        beatScale = 1.0","                    }","                }","            }","    }","}"],"syntax":"swift"},{"type":"heading","level":2,"text":"Debugging Real-Time Systems","anchor":"Debugging-Real-Time-Systems"},{"type":"heading","level":3,"text":"Performance Monitoring","anchor":"Performance-Monitoring"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Track real-time performance metrics:"}]},{"type":"codeListing","code":["class RealTimePerformanceMonitor {","    private var audioCallbackTimes: [CFAbsoluteTime] = []","    private var uiUpdateTimes: [CFAbsoluteTime] = []","    private var dropoutCount: Int = 0","    ","    func recordAudioCallback() {","        let time = CFAbsoluteTimeGetCurrent()","        audioCallbackTimes.append(time)","        ","        \/\/ Keep only recent samples","        if audioCallbackTimes.count > 1000 {","            audioCallbackTimes.removeFirst(100)","        }","    }","    ","    func recordUIUpdate() {","        let time = CFAbsoluteTimeGetCurrent()","        uiUpdateTimes.append(time)","        ","        if uiUpdateTimes.count > 1000 {","            uiUpdateTimes.removeFirst(100)","        }","    }","    ","    func recordDropout() {","        dropoutCount += 1","    }","    ","    func generateReport() -> PerformanceReport {","        let avgAudioInterval = calculateAverageInterval(audioCallbackTimes)","        let avgUIInterval = calculateAverageInterval(uiUpdateTimes)","        ","        return PerformanceReport(","            audioCallbackRate: 1.0 \/ avgAudioInterval,","            uiUpdateRate: 1.0 \/ avgUIInterval,","            dropoutCount: dropoutCount,","            isStable: dropoutCount < 5 && avgAudioInterval < 0.01","        )","    }","}"],"syntax":"swift"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Real-time audio interfaces require careful attention to thread management, efficient updates, and minimal latency patterns. AudioUI provides the tools and patterns needed to build professional-grade real-time audio applications."}]}]}],"metadata":{"title":"Real-Time Audio","modules":[{"name":"AudioUI"}],"role":"collectionGroup"},"kind":"article","schemaVersion":{"major":0,"minor":3,"patch":0},"seeAlsoSections":[{"identifiers":["doc:\/\/audiouiswift.AudioUI\/documentation\/AudioUI\/PerformanceOptimization","doc:\/\/audiouiswift.AudioUI\/documentation\/AudioUI\/MetalEffects","doc:\/\/audiouiswift.AudioUI\/documentation\/AudioUI\/CrossPlatformConsiderations"],"title":"Performance & Platform","generated":true,"anchor":"Performance--Platform"}],"hierarchy":{"paths":[["doc:\/\/audiouiswift.AudioUI\/documentation\/AudioUI"]]},"abstract":[{"type":"text","text":"Design AudioUI interfaces that respond seamlessly to live audio processing while maintaining professional performance standards."}],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/audiouiswift.AudioUI\/documentation\/AudioUI\/RealTimeAudio"},"references":{"doc://audiouiswift.AudioUI/documentation/AudioUI/MetalEffects":{"identifier":"doc:\/\/audiouiswift.AudioUI\/documentation\/AudioUI\/MetalEffects","type":"topic","role":"collectionGroup","title":"Metal Effects","abstract":[{"type":"text","text":"Leverage GPU acceleration to create stunning real-time visual effects for advanced audio applications using AudioUI’s Metal-powered rendering system."}],"url":"\/documentation\/audioui\/metaleffects","kind":"article"},"doc://audiouiswift.AudioUI/documentation/AudioUI/CrossPlatformConsiderations":{"title":"Cross-Platform Considerations","type":"topic","role":"article","kind":"article","abstract":[{"type":"text","text":"Building audio interfaces that work seamlessly across iOS, macOS, and beyond."}],"url":"\/documentation\/audioui\/crossplatformconsiderations","identifier":"doc:\/\/audiouiswift.AudioUI\/documentation\/AudioUI\/CrossPlatformConsiderations"},"doc://audiouiswift.AudioUI/documentation/AudioUI":{"kind":"symbol","abstract":[{"type":"text","text":"The ultimate SwiftUI framework for professional audio interface development."}],"identifier":"doc:\/\/audiouiswift.AudioUI\/documentation\/AudioUI","role":"collection","type":"topic","title":"AudioUI","url":"\/documentation\/audioui"},"doc://audiouiswift.AudioUI/documentation/AudioUI/PerformanceOptimization":{"role":"collectionGroup","kind":"article","identifier":"doc:\/\/audiouiswift.AudioUI\/documentation\/AudioUI\/PerformanceOptimization","title":"Performance Optimization","type":"topic","url":"\/documentation\/audioui\/performanceoptimization","abstract":[{"text":"Advanced techniques and best practices for building high-performance audio interfaces with AudioUI that maintain smooth 60fps rendering and minimal CPU usage.","type":"text"}]}}}